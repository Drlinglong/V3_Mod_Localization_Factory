# 使用 Ollama 进行本地化翻译

本文档将指导您如何设置并在“Paradox Mod 本地化工厂”中使用 [Ollama](https://ollama.com/) 来运行本地大语言模型（LLM）进行翻译。

## 为什么选择 Ollama？

- **隐私与安全**：所有翻译都在您的本地计算机上进行，确保了源代码和内容的隐私。
- **离线工作**：一旦模型下载完成，您可以在没有互联网连接的情况下进行翻译。
- **成本效益**：使用本地模型无需支付API调用费用。
- **高度可定制**：您可以选择并运行各种针对特定任务优化的开源模型。

## 设置步骤

### 1. 安装 Ollama

首先，您需要在您的计算机上安装 Ollama。

- 访问 [Ollama 官方网站](https://ollama.com/)。
- 根据您的操作系统（Windows, macOS, 或 Linux）下载并安装相应的程序。
- 安装程序会自动将 Ollama 设置为后台服务。

### 2. 下载一个语言模型

安装完 Ollama 后，您需要下载一个用于翻译的语言模型。我们推荐使用功能全面且性能优越的模型，例如 `llama3` 或 `qwen`。

打开您的终端（在 Windows 上是命令提示符或 PowerShell），然后运行以下命令：

```bash
ollama pull llama3
```

或者，如果您想使用其他模型，例如通义千问（Qwen）：

```bash
ollama pull qwen
```

模型的下载可能需要一些时间，具体取决于您的网络速度和模型大小。您可以通过 `ollama list` 命令查看已下载的所有模型。

### 3. 在程序中配置 Ollama

本程序已预设好与本地 Ollama 服务的连接。

- **默认地址**：程序默认会尝试连接 `http://localhost:11434`。对于大多数标准安装的 Ollama，您无需进行任何额外配置。
- **选择模型**：在程序的翻译设置界面，从“翻译服务商”下拉菜单中选择 **Ollama**。程序会自动加载您已下载的模型列表，您可以从中选择一个用于翻译。

### 4. （高级）自定义 Ollama 服务地址

如果您的 Ollama 服务运行在不同的 IP 地址或端口上（例如，在另一台局域网内的机器上），您可以通过设置环境变量来指定连接地址。

- 设置一个名为 `OLLAMA_BASE_URL` 的环境变量。
- 将其值设置为您的 Ollama 服务的完整 URL。

**示例：**

- 在 Windows 上：
  ```cmd
  set OLLAMA_BASE_URL=http://192.168.1.100:11434
  ```
- 在 macOS 或 Linux 上：
  ```bash
  export OLLAMA_BASE_URL=http://192.168.1.100:11434
  ```

设置完成后，重新启动本程序，它将自动使用您指定的新地址。

---

现在您已经准备好使用 Ollama 在本地进行安全、高效的 Mod 本地化翻译了！