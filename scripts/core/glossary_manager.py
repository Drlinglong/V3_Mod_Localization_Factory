# scripts/core/glossary_manager.py
import os
import json
import logging
import re
from typing import Dict, List, Set, Optional, Tuple
from scripts.config import PROJECT_ROOT


class GlossaryManager:
    """æ¸¸æˆä¸“ç”¨è¯å…¸ç®¡ç†å™¨"""
    
    def __init__(self):
        self.glossaries: Dict[str, Dict] = {}
        self.current_game_glossary: Optional[Dict] = None
        self.current_game_id: Optional[str] = None
        self.current_auxiliary_glossaries: List[Dict] = []  # å¤–æŒ‚è¯å…¸åˆ—è¡¨
        self.merged_glossary: Optional[Dict] = None  # åˆå¹¶åçš„è¯å…¸
        self.fuzzy_matching_mode: str = 'loose'  # æ¨¡ç³ŠåŒ¹é…æ¨¡å¼: 'strict' æˆ– 'loose'
        
    def scan_auxiliary_glossaries(self, game_id: str) -> List[Dict]:
        """
        æ‰«ææŒ‡å®šæ¸¸æˆç›®å½•ä¸‹çš„å¤–æŒ‚è¯å…¸
        
        Args:
            game_id: æ¸¸æˆID
            
        Returns:
            List[Dict]: å¤–æŒ‚è¯å…¸ä¿¡æ¯åˆ—è¡¨
        """
        glossary_dir = os.path.join(PROJECT_ROOT, 'data', 'glossary', game_id)
        if not os.path.exists(glossary_dir):
            return []
            
        auxiliary_glossaries = []
        
        for filename in os.listdir(glossary_dir):
            if filename.endswith('.json') and filename != 'glossary.json':
                file_path = os.path.join(glossary_dir, filename)
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        glossary_data = json.load(f)
                        
                    # æå–è¯å…¸ä¿¡æ¯
                    metadata = glossary_data.get('metadata', {})
                    auxiliary_glossaries.append({
                        'filename': filename,
                        'file_path': file_path,
                        'name': metadata.get('name', filename.replace('.json', '')),
                        'description': metadata.get('description', ''),
                        'entry_count': len(glossary_data.get('entries', [])),
                        'data': glossary_data
                    })
                except Exception as e:
                    logging.warning(f"Failed to load auxiliary glossary {filename}: {e}")
                    
        return auxiliary_glossaries
    
    def load_game_glossary(self, game_id: str) -> bool:
        """
        åŠ è½½æŒ‡å®šæ¸¸æˆçš„ä¸»è¯å…¸æ–‡ä»¶
        
        Args:
            game_id: æ¸¸æˆID (å¦‚ 'victoria3', 'stellaris')
            
        Returns:
            bool: æ˜¯å¦æˆåŠŸåŠ è½½
        """
        if game_id == self.current_game_id and self.current_game_glossary:
            return True
            
        glossary_path = os.path.join(PROJECT_ROOT, 'data', 'glossary', game_id, 'glossary.json')
        
        try:
            if os.path.exists(glossary_path):
                with open(glossary_path, 'r', encoding='utf-8') as f:
                    self.current_game_glossary = json.load(f)
                    self.current_game_id = game_id
                    from scripts.utils import i18n
                    logging.info(i18n.t("glossary_loaded_success", 
                                      game_id=game_id, 
                                      count=len(self.current_game_glossary.get('entries', []))))
                    return True
            else:
                from scripts.utils import i18n
                logging.warning(i18n.t("glossary_file_not_found", path=glossary_path))
                self.current_game_glossary = None
                self.current_game_id = game_id
                return False
                
        except Exception as e:
            from scripts.utils import i18n
            logging.error(i18n.t("glossary_load_failed", error=str(e)))
            self.current_game_glossary = None
            self.current_game_id = game_id
            return False
    
    def has_any_glossary(self) -> bool:
        """
        æ£€æŸ¥æ˜¯å¦æœ‰ä»»ä½•å¯ç”¨çš„è¯å…¸ï¼ˆä¸»è¯å…¸æˆ–å¤–æŒ‚è¯å…¸ï¼‰
        
        Returns:
            bool: æ˜¯å¦æœ‰ä»»ä½•è¯å…¸å¯ç”¨
        """
        return (self.current_game_glossary is not None or 
                len(self.current_auxiliary_glossaries) > 0 or 
                self.merged_glossary is not None)
    
    def get_glossary_status_summary(self) -> str:
        """
        è·å–è¯å…¸çŠ¶æ€æ‘˜è¦
        
        Returns:
            str: è¯å…¸çŠ¶æ€æè¿°
        """
        if not self.current_game_id:
            return "æœªåŠ è½½ä»»ä½•è¯å…¸"
            
        if self.merged_glossary:
            main_count = len(self.current_game_glossary.get('entries', [])) if self.current_game_glossary else 0
            aux_count = len(self.current_auxiliary_glossaries)
            total_count = len(self.merged_glossary.get('entries', []))
            return f"ä¸»è¯å…¸({main_count}æ¡) + å¤–æŒ‚è¯å…¸({aux_count}ä¸ª) = æ€»è®¡({total_count}æ¡)"
        elif self.current_game_glossary:
            count = len(self.current_game_glossary.get('entries', []))
            return f"ä»…ä¸»è¯å…¸({count}æ¡)"
        elif self.current_auxiliary_glossaries:
            aux_count = len(self.current_auxiliary_glossaries)
            return f"ä»…å¤–æŒ‚è¯å…¸({aux_count}ä¸ª)"
        else:
            return "æ— å¯ç”¨è¯å…¸"
    
    def load_auxiliary_glossaries(self, selected_indices: List[int]) -> bool:
        """
        åŠ è½½é€‰ä¸­çš„å¤–æŒ‚è¯å…¸
        
        Args:
            selected_indices: é€‰ä¸­çš„å¤–æŒ‚è¯å…¸ç´¢å¼•åˆ—è¡¨
            
        Returns:
            bool: æ˜¯å¦æˆåŠŸåŠ è½½
        """
        if not self.current_game_id:
            return False
            
        auxiliary_glossaries = self.scan_auxiliary_glossaries(self.current_game_id)
        self.current_auxiliary_glossaries = []
        
        for idx in selected_indices:
            if 0 <= idx < len(auxiliary_glossaries):
                self.current_auxiliary_glossaries.append(auxiliary_glossaries[idx])
                
        # åˆå¹¶è¯å…¸
        self._merge_glossaries()
        return True
    
    def _merge_glossaries(self):
        """åˆå¹¶ä¸»è¯å…¸å’Œå¤–æŒ‚è¯å…¸"""
        if not self.current_game_glossary:
            self.merged_glossary = None
            return
            
        # æ·±æ‹·è´ä¸»è¯å…¸
        merged = json.loads(json.dumps(self.current_game_glossary))
        merged_entries = merged.get('entries', [])
        
        # æ·»åŠ å¤–æŒ‚è¯å…¸æ¡ç›®
        for aux_glossary in self.current_auxiliary_glossaries:
            aux_entries = aux_glossary['data'].get('entries', [])
            for entry in aux_entries:
                # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ç›¸åŒIDçš„æ¡ç›®
                existing_entry = next((e for e in merged_entries if e.get('id') == entry.get('id')), None)
                if existing_entry:
                    # å¦‚æœå­˜åœ¨ï¼Œå¤–æŒ‚è¯å…¸çš„æ¡ç›®ä¼˜å…ˆçº§æ›´é«˜
                    existing_entry.update(entry)
                else:
                    # å¦‚æœä¸å­˜åœ¨ï¼Œç›´æ¥æ·»åŠ 
                    merged_entries.append(entry)
                    
        self.merged_glossary = merged
        
        # è®°å½•åˆå¹¶ç»“æœ
        total_entries = len(merged_entries)
        main_entries = len(self.current_game_glossary.get('entries', []))
        aux_entries = total_entries - main_entries
        
        from scripts.utils import i18n
        if i18n.get_current_language() == "en_US":
            logging.info(f"Glossary merged: {main_entries} main entries + {aux_entries} auxiliary entries = {total_entries} total")
        else:
            logging.info(f"è¯å…¸åˆå¹¶å®Œæˆ: {main_entries} ä¸ªä¸»è¯å…¸æ¡ç›® + {aux_entries} ä¸ªå¤–æŒ‚è¯å…¸æ¡ç›® = {total_entries} ä¸ªæ€»è®¡")
    
    def get_glossary_for_translation(self) -> Optional[Dict]:
        """è·å–ç”¨äºç¿»è¯‘çš„è¯å…¸ï¼ˆä¼˜å…ˆä½¿ç”¨åˆå¹¶åçš„è¯å…¸ï¼‰"""
        return self.merged_glossary if self.merged_glossary else self.current_game_glossary
    
    def extract_relevant_terms(self, texts: List[str], source_lang: str, target_lang: str) -> List[Dict]:
        """
        ä»å¾…ç¿»è¯‘æ–‡æœ¬ä¸­æå–ç›¸å…³çš„è¯å…¸æœ¯è¯­ï¼ˆæ”¯æŒåŒå‘è¯†åˆ«ã€å˜ä½“æŸ¥æ‰¾å’Œæ™ºèƒ½åŒ¹é…ï¼‰
        
        Args:
            texts: å¾…ç¿»è¯‘çš„æ–‡æœ¬åˆ—è¡¨
            source_lang: æºè¯­è¨€ä»£ç 
            target_lang: ç›®æ ‡è¯­è¨€ä»£ç 
            
        Returns:
            List[Dict]: ç›¸å…³æœ¯è¯­åˆ—è¡¨
        """
        glossary = self.get_glossary_for_translation()
        if not glossary:
            return []
            
        relevant_terms = []
        all_text = " ".join(texts).lower()
        
        # ä½¿ç”¨æ™ºèƒ½åŒ¹é…ç®—æ³•
        matches = self._smart_term_matching(all_text, source_lang, target_lang)
        
        # è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
        for match in matches:
            relevant_terms.append({
                'translations': {
                    source_lang: match['source_term'],
                    target_lang: match['target_term']
                },
                'id': match['id'],
                'metadata': match['metadata'],
                'variants': match.get('variants', {}),
                'match_type': match['match_type'],  # æ–°å¢ï¼šåŒ¹é…ç±»å‹
                'confidence': match['confidence']   # æ–°å¢ï¼šåŒ¹é…ç½®ä¿¡åº¦
            })
        
        # æŒ‰ç½®ä¿¡åº¦å’Œæœ¯è¯­é•¿åº¦æ’åº
        relevant_terms.sort(key=lambda x: (x['confidence'], len(x['translations'][source_lang])), reverse=True)
        
        from scripts.utils import i18n
        logging.info(i18n.t("glossary_terms_extracted", 
                           count=len(relevant_terms), 
                           text_count=len(texts)))
        return relevant_terms
    
    def _smart_term_matching(self, text: str, source_lang: str, target_lang: str) -> List[Dict]:
        """
        æ™ºèƒ½æœ¯è¯­åŒ¹é…ï¼Œæ”¯æŒå¤šç§åŒ¹é…æ¨¡å¼
        
        Args:
            text: å¾…åŒ¹é…çš„æ–‡æœ¬
            source_lang: æºè¯­è¨€ä»£ç 
            target_lang: ç›®æ ‡è¯­è¨€ä»£ç 
            
        Returns:
            List[Dict]: åŒ¹é…ç»“æœåˆ—è¡¨
        """
        matches = []
        glossary = self.get_glossary_for_translation()
        
        if not glossary:
            return matches
        
        for entry in glossary.get('entries', []):
            translations = entry.get('translations', {})
            source_term = translations.get(source_lang, "")
            target_term = translations.get(target_lang, "")
            
            if not source_term or not target_term:
                continue
            
            # 1. ç²¾ç¡®åŒ¹é…ï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼‰
            if source_term.lower() in text:
                matches.append({
                    'source_term': source_term,
                    'target_term': target_term,
                    'id': entry.get('id', ''),
                    'metadata': entry.get('metadata', {}),
                    'variants': entry.get('variants', {}),
                    'match_type': 'exact',
                    'confidence': 1.0
                })
                continue
            
            # 2. å˜ä½“åŒ¹é…ï¼ˆåŒ…æ‹¬ç®€ç§°ã€åŒä¹‰è¯ç­‰ï¼‰
            variants = entry.get('variants', {}).get(source_lang, [])
            for variant in variants:
                if variant.lower() in text:
                    matches.append({
                        'source_term': source_term,
                        'target_term': target_term,
                        'id': entry.get('id', ''),
                        'metadata': entry.get('metadata', {}),
                        'variants': entry.get('variants', {}),
                        'match_type': 'variant',
                        'confidence': 0.9
                    })
                    break
            
            # 3. ç®€ç§°åŒ¹é…ï¼ˆå¦‚æœæœ‰abbreviationså­—æ®µï¼‰
            abbreviations = entry.get('abbreviations', {}).get(source_lang, [])
            for abbreviation in abbreviations:
                if abbreviation.lower() in text:
                    matches.append({
                        'source_term': source_term,
                        'target_term': target_term,
                        'id': entry.get('id', ''),
                        'metadata': entry.get('metadata', {}),
                        'variants': entry.get('variants', {}),
                        'match_type': 'abbreviation',
                        'confidence': 0.85  # ç®€ç§°åŒ¹é…ï¼Œç½®ä¿¡åº¦ç¨é«˜
                    })
                    break
            
            # 4. æ™ºèƒ½éƒ¨åˆ†åŒ¹é…ï¼ˆè‡ªåŠ¨è¯†åˆ«ç®€ç§°å’Œå…¨ç§°å…³ç³»ï¼‰
            partial_match = self._check_partial_match(source_term, text, source_lang)
            if partial_match:
                # æ£€æŸ¥æ˜¯å¦æ˜¯æ¨¡ç³ŠåŒ¹é…
                if partial_match.get('match_type') == 'fuzzy':
                    match_type = 'fuzzy'
                else:
                    match_type = 'partial'
                
                matches.append({
                    'source_term': source_term,
                    'target_term': target_term,
                    'id': entry.get('id', ''),
                    'metadata': entry.get('metadata', {}),
                    'variants': entry.get('variants', {}),
                    'match_type': match_type,
                    'confidence': partial_match['confidence']
                })
        
        return self._deduplicate_matches(matches)
    
    def _check_partial_match(self, source_term: str, text: str, source_lang: str) -> Optional[Dict]:
        """
        æ™ºèƒ½éƒ¨åˆ†åŒ¹é…ï¼Œè¯†åˆ«ç®€ç§°å’Œå…¨ç§°å…³ç³»ï¼Œæ”¯æŒæ¨¡ç³ŠåŒ¹é…
        å¦‚ï¼š"ç™¾é¬¼å¤œè¡Œ" åŒ¹é… "ç™¾é¬¼å¤œè¡Œè”åˆå­¦å›­"
        å¦‚ï¼š"å°æ—¥æœ¬å¸å›½è”é‚¦èˆ°é˜Ÿ" æ¨¡ç³ŠåŒ¹é… "å°æ—¥æœ¬å¸å›½è”åˆèˆ°é˜Ÿ"
        """
        # 1. ç²¾ç¡®éƒ¨åˆ†åŒ¹é…ï¼ˆå½“å‰å®ç°ï¼‰
        if len(source_term) > 3 and source_term.lower() in text:
            match_ratio = len(source_term) / len(text)
            if match_ratio > 0.3:
                return {
                    'confidence': 0.7 + (match_ratio * 0.2)
                }
        
        # 2. æ¨¡ç³ŠåŒ¹é…ï¼ˆæ–°å¢åŠŸèƒ½ï¼‰
        fuzzy_match = self._check_fuzzy_match(source_term, text, source_lang)
        if fuzzy_match:
            return fuzzy_match
        
        return None
    
    def _check_fuzzy_match(self, source_term: str, text: str, source_lang: str) -> Optional[Dict]:
        """
        æ¨¡ç³ŠåŒ¹é…ï¼Œå®¹å¿æ‹¼å†™é”™è¯¯å’Œè½»å¾®å·®å¼‚
        """
        # ä¸¥æ ¼æ¨¡å¼ä¸‹ç¦ç”¨æ¨¡ç³ŠåŒ¹é…
        if self.fuzzy_matching_mode == 'strict':
            return None
            
        # æ ¹æ®è¯­è¨€ç±»å‹è¿›è¡Œæ™ºèƒ½åˆ†è¯
        text_tokens = self._tokenize_text(text, source_lang)
        source_tokens = self._tokenize_text(source_term, source_lang)
        
        if len(source_tokens) < 2:
            return None
        
        # è®¡ç®—å•è¯/å­—ç¬¦çº§åˆ«çš„åŒ¹é…åº¦
        matched_tokens = 0
        total_source_tokens = len(source_tokens)
        
        for source_token in source_tokens:
            if len(source_token) < 2:  # å¿½ç•¥å¤ªçŸ­çš„è¯
                continue
                
            # æ£€æŸ¥æ˜¯å¦æœ‰ç›¸ä¼¼å•è¯/å­—ç¬¦
            for text_token in text_tokens:
                if len(text_token) < 2:
                    continue
                
                # ç²¾ç¡®åŒ¹é…
                if source_token == text_token:
                    matched_tokens += 1
                    break
                
                # æ¨¡ç³ŠåŒ¹é…ï¼ˆç¼–è¾‘è·ç¦»ï¼‰
                if self._is_similar_word(source_token, text_token):
                    matched_tokens += 1
                    break
        
        # è®¡ç®—åŒ¹é…åº¦
        if matched_tokens > 0:
            match_ratio = matched_tokens / total_source_tokens
            if match_ratio > 0.5:  # è¶…è¿‡50%çš„å•è¯åŒ¹é…
                # è°ƒæ•´ç½®ä¿¡åº¦èŒƒå›´åˆ°0.3-0.6
                confidence = 0.3 + (match_ratio * 0.3)
                return {
                    'confidence': confidence,
                    'match_type': 'fuzzy'
                }
        
        return None
    
    def _tokenize_text(self, text: str, lang: str) -> List[str]:
        """
        æ ¹æ®è¯­è¨€ç±»å‹è¿›è¡Œæ™ºèƒ½åˆ†è¯
        """
        if lang in ['zh-CN', 'zh-TW', 'ja', 'ko']:
            # ä¸­æ–‡ã€æ—¥æ–‡ã€éŸ©æ–‡ï¼šæŒ‰å­—ç¬¦åˆ†å‰²
            return list(text)
        else:
            # è‹±æ–‡ç­‰ï¼šæŒ‰ç©ºæ ¼å’Œæ ‡ç‚¹åˆ†å‰²
            return re.findall(r'\w+', text.lower())
    
    def _is_similar_word(self, word1: str, word2: str) -> bool:
        """
        æ£€æŸ¥ä¸¤ä¸ªå•è¯æ˜¯å¦ç›¸ä¼¼ï¼ˆä½¿ç”¨ç¼–è¾‘è·ç¦»ï¼‰
        """
        if len(word1) < 3 or len(word2) < 3:
            return False
        
        # è®¡ç®—ç¼–è¾‘è·ç¦»
        distance = self._levenshtein_distance(word1, word2)
        
        # å…è®¸çš„ç¼–è¾‘è·ç¦»ï¼šå•è¯è¶Šé•¿ï¼Œå…è®¸çš„å·®å¼‚è¶Šå¤§
        max_distance = max(1, len(word1) // 4)
        
        return distance <= max_distance
    
    def _levenshtein_distance(self, s1: str, s2: str) -> int:
        """
        è®¡ç®—ä¸¤ä¸ªå­—ç¬¦ä¸²çš„ç¼–è¾‘è·ç¦»
        """
        if len(s1) < len(s2):
            return self._levenshtein_distance(s2, s1)
        
        if len(s2) == 0:
            return len(s1)
        
        previous_row = list(range(len(s2) + 1))
        for i, c1 in enumerate(s1):
            current_row = [i + 1]
            for j, c2 in enumerate(s2):
                insertions = previous_row[j + 1] + 1
                deletions = current_row[j] + 1
                substitutions = previous_row[j] + (c1 != c2)
                current_row.append(min(insertions, deletions, substitutions))
            previous_row = current_row
        
        return previous_row[-1]
    
    def _deduplicate_matches(self, matches: List[Dict]) -> List[Dict]:
        """
        å»é‡åŒ¹é…ç»“æœï¼Œä¿ç•™æœ€é«˜ç½®ä¿¡åº¦çš„åŒ¹é…
        """
        # æŒ‰IDåˆ†ç»„ï¼Œä¿ç•™æœ€é«˜ç½®ä¿¡åº¦çš„åŒ¹é…
        unique_matches = {}
        for match in matches:
            match_id = match['id']
            if match_id not in unique_matches or match['confidence'] > unique_matches[match_id]['confidence']:
                unique_matches[match_id] = match
        
        return list(unique_matches.values())
    
    def _term_appears_in_text(self, term: str, text: str, source_lang: str) -> bool:
        """
        æ£€æŸ¥æœ¯è¯­æ˜¯å¦åœ¨æ–‡æœ¬ä¸­å‡ºç°ï¼ˆæ”¯æŒå˜ä½“å’Œå¤šè¯­è¨€ï¼‰
        
        Args:
            term: æœ¯è¯­
            text: æ–‡æœ¬
            source_lang: æºè¯­è¨€ä»£ç 
            
        Returns:
            bool: æ˜¯å¦å‡ºç°
        """
        # ç›´æ¥åŒ¹é…
        if term.lower() in text:
            return True
            
        # æ£€æŸ¥å˜ä½“ï¼ˆæ”¯æŒå¤šè¯­è¨€å˜ä½“ï¼‰
        glossary = self.get_glossary_for_translation()
        if glossary:
            for entry in glossary.get('entries', []):
                if entry.get('translations', {}).get(source_lang, '').lower() == term.lower():
                    variants = entry.get('variants', {}).get(source_lang, [])
                    for variant in variants:
                        if variant.lower() in text:
                            return True
                            
        return False
    
    def create_dynamic_glossary_prompt(self, relevant_terms: List[Dict], source_lang: str, target_lang: str) -> str:
        """
        åˆ›å»ºåŠ¨æ€è¯å…¸æç¤ºï¼Œç”¨äºæ³¨å…¥åˆ°AIç¿»è¯‘è¯·æ±‚ä¸­
        
        Args:
            relevant_terms: ç›¸å…³æœ¯è¯­åˆ—è¡¨
            source_lang: æºè¯­è¨€ä»£ç 
            target_lang: ç›®æ ‡è¯­è¨€ä»£ç 
            
        Returns:
            str: æ ¼å¼åŒ–çš„è¯å…¸æç¤º
        """
        if not relevant_terms:
            return ""
            
        prompt_lines = [
            "ğŸ” CRITICAL GLOSSARY INSTRUCTIONS - HIGH PRIORITY ğŸ”",
            f"The following terms must be translated strictly according to the glossary to maintain consistency:",
            "",
            "Glossary Reference:"
        ]
        
        for term in relevant_terms:
            source = term['translations'][source_lang]
            target = term['translations'][target_lang]
            metadata = term.get('metadata', {})
            remarks = metadata.get('remarks', '')
            variants = term.get('variants', {}).get(source_lang, [])
            match_type = term.get('match_type', 'unknown')
            confidence = term.get('confidence', 1.0)
            
            # æ˜¾ç¤ºåŒ¹é…ç±»å‹å’Œç½®ä¿¡åº¦
            match_info = f"[{match_type.upper()}]"
            if confidence < 1.0:
                match_info += f" (confidence: {confidence:.1f})"
            
            prompt_lines.append(f"â€¢ {match_info} '{source}' â†’ '{target}'")
            
            # å¦‚æœæœ‰å˜ä½“è¯æ±‡ï¼Œåœ¨æç¤ºä¸­è¯´æ˜
            if variants:
                variant_list = ", ".join([f"'{v}'" for v in variants])
                prompt_lines.append(f"  Variants: {variant_list}")
                
            if remarks:
                prompt_lines.append(f"  Remarks: {remarks}")
        
        # æ·»åŠ åŒ¹é…ç±»å‹è¯´æ˜
        prompt_lines.extend([
            "",
            "Match Type Explanation:",
            "â€¢ EXACT: Exact match, highest priority",
            "â€¢ VARIANT: Variant match, such as plural forms, synonyms, abbreviations",
            "â€¢ ABBREVIATION: Abbreviation match, such as organization abbreviations, person name abbreviations",
            "â€¢ PARTIAL: Smart partial match, automatically identifies abbreviation and full name relationships",
            "â€¢ FUZZY: Fuzzy match, tolerates spelling errors and minor differences",
            "",
            "Translation Requirements:",
            "1. The above terms must be translated strictly according to the glossary, no arbitrary changes allowed",
            "2. Variant forms of terms should also be translated to the same target language equivalent",
            "3. For abbreviation and partial match terms, choose the most appropriate translation based on context",
            "4. For fuzzy match terms, translate reasonably based on context and glossary",
            "5. Maintain consistency and accuracy of game terminology",
            "6. Term placement in sentences should be natural and appropriate",
            "7. If encountering terms not in the glossary, translate reasonably based on context",
            "",
            "Please ensure strict adherence to the above glossary reference during translation."
        ])
        
        return "\n".join(prompt_lines)
        
        # ä¸­æ–‡ç‰ˆæœ¬æ³¨é‡Šï¼ˆä»…ä¾›å‚è€ƒï¼‰:
        # prompt_lines = [
        #     "ğŸ” å…³é”®è¯å…¸æŒ‡ç¤º - é«˜ä¼˜å…ˆçº§ ğŸ”",
        #     f"ä»¥ä¸‹æœ¯è¯­å¿…é¡»ä¸¥æ ¼æŒ‰ç…§è¯å…¸ç¿»è¯‘ï¼Œä¿æŒæ¸¸æˆæœ¯è¯­çš„ä¸€è‡´æ€§ï¼š",
        #     "",
        #     "æœ¯è¯­å¯¹ç…§è¡¨ï¼š"
        # ]
        # 
        # for term in relevant_terms:
        #     source = term['translations'][source_lang]
        #     target = term['translations'][target_lang]
        #     metadata = term.get('metadata', {})
        #     remarks = metadata.get('remarks', '')
        #     variants = term.get('variants', {}).get(source_lang, [])
        #     match_type = term.get('match_type', 'unknown')
        #     confidence = term.get('confidence', 1.0)
        #     
        #     # æ˜¾ç¤ºåŒ¹é…ç±»å‹å’Œç½®ä¿¡åº¦
        #     match_info = f"[{match_type.upper()}]"
        #     if confidence < 1.0:
        #         match_info += f" (ç½®ä¿¡åº¦: {confidence:.1f})"
        #     
        #     prompt_lines.append(f"â€¢ {match_info} '{source}' â†’ '{target}'")
        #     
        #     # å¦‚æœæœ‰å˜ä½“è¯æ±‡ï¼Œåœ¨æç¤ºä¸­è¯´æ˜
        #     if variants:
        #         variant_list = ", ".join([f"'{v}'" for v in variants])
        #         prompt_lines.append(f"  å˜ä½“: {variant_list}")
        #             
        #     if remarks:
        #         prompt_lines.append(f"  å¤‡æ³¨: {remarks}")
        # 
        # # æ·»åŠ åŒ¹é…ç±»å‹è¯´æ˜
        # prompt_lines.extend([
        #     "",
        #     "åŒ¹é…ç±»å‹è¯´æ˜ï¼š",
        #     "â€¢ EXACT: ç²¾ç¡®åŒ¹é…ï¼Œæœ€é«˜ä¼˜å…ˆçº§",
        #     "â€¢ VARIANT: å˜ä½“åŒ¹é…ï¼Œå¦‚å¤æ•°å½¢å¼ã€åŒä¹‰è¯ã€ç®€ç§°",
        #     "â€¢ ABBREVIATION: ç®€ç§°åŒ¹é…ï¼Œå¦‚ç»„ç»‡ç®€ç§°ã€äººåç®€ç§°",
        #     "â€¢ PARTIAL: æ™ºèƒ½éƒ¨åˆ†åŒ¹é…ï¼Œè‡ªåŠ¨è¯†åˆ«ç®€ç§°å’Œå…¨ç§°å…³ç³»",
        #     "â€¢ FUZZY: æ¨¡ç³ŠåŒ¹é…ï¼Œå®¹å¿æ‹¼å†™é”™è¯¯å’Œè½»å¾®å·®å¼‚",
        #     "",
        #     "ç¿»è¯‘è¦æ±‚ï¼š",
        #     "1. ä¸Šè¿°æœ¯è¯­å¿…é¡»ä¸¥æ ¼æŒ‰ç…§è¯å…¸ç¿»è¯‘ï¼Œä¸å¾—éšæ„æ›´æ”¹",
        #     "2. æœ¯è¯­çš„å˜ä½“å½¢å¼ä¹Ÿåº”è¯¥ç¿»è¯‘æˆç›¸åŒçš„ä¸­æ–‡",
        #     "3. ç®€ç§°å’Œéƒ¨åˆ†åŒ¹é…çš„æœ¯è¯­ï¼Œè¯·æ ¹æ®ä¸Šä¸‹æ–‡é€‰æ‹©æœ€åˆé€‚çš„ç¿»è¯‘",
        #     "4. æ¨¡ç³ŠåŒ¹é…çš„æœ¯è¯­ï¼Œè¯·æ ¹æ®ä¸Šä¸‹æ–‡å’Œè¯å…¸è¿›è¡Œåˆç†ç¿»è¯‘",
        #     "5. ä¿æŒæ¸¸æˆæœ¯è¯­çš„ä¸€è‡´æ€§å’Œå‡†ç¡®æ€§",
        #     "6. æœ¯è¯­åœ¨å¥å­ä¸­çš„ä½ç½®åº”è¯¥è‡ªç„¶ã€æ°å½“",
        #     "7. å¦‚æœé‡åˆ°è¯å…¸ä¸­æœªåŒ…å«çš„æœ¯è¯­ï¼Œè¯·æ ¹æ®ä¸Šä¸‹æ–‡è¿›è¡Œåˆç†ç¿»è¯‘",
        #     "",
        #     "è¯·ç¡®ä¿åœ¨ç¿»è¯‘è¿‡ç¨‹ä¸­ä¸¥æ ¼éµå¾ªä»¥ä¸Šæœ¯è¯­å¯¹ç…§è¡¨ã€‚"
        # ])
    
    def get_glossary_stats(self) -> Dict:
        """
        è·å–å½“å‰åŠ è½½è¯å…¸çš„ç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            Dict: ç»Ÿè®¡ä¿¡æ¯
        """
        if not self.current_game_glossary:
            return {"loaded": False, "game_id": self.current_game_id}
            
        entries = self.current_game_glossary.get('entries', [])
        metadata = self.current_game_glossary.get('metadata', {})
        
        return {
            "loaded": True,
            "game_id": self.current_game_id,
            "total_entries": len(entries),
            "description": metadata.get('description', ''),
            "last_updated": metadata.get('last_updated', ''),
            "sources": metadata.get('sources', []),
            "auxiliary_count": len(self.current_auxiliary_glossaries)
        }
    
    def get_auxiliary_glossaries_info(self) -> List[Dict]:
        """è·å–å¤–æŒ‚è¯å…¸ä¿¡æ¯åˆ—è¡¨"""
        if not self.current_game_id:
            return []
        return self.scan_auxiliary_glossaries(self.current_game_id)
    
    def set_fuzzy_matching_mode(self, mode: str):
        """
        è®¾ç½®æ¨¡ç³ŠåŒ¹é…æ¨¡å¼
        
        Args:
            mode: æ¨¡å¼ç±»å‹ ('strict' æˆ– 'loose')
        """
        if mode in ['strict', 'loose']:
            self.fuzzy_matching_mode = mode
            from scripts.utils import i18n
            mode_name = "ä¸¥æ ¼æ¨¡å¼" if mode == 'strict' else "å®½æ¾æ¨¡å¼"
            logging.info(i18n.t("fuzzy_mode_set", mode=mode_name))
        else:
            logging.warning(f"Invalid fuzzy matching mode: {mode}. Using default 'loose' mode.")
            self.fuzzy_matching_mode = 'loose'


# å…¨å±€è¯å…¸ç®¡ç†å™¨å®ä¾‹
glossary_manager = GlossaryManager()
