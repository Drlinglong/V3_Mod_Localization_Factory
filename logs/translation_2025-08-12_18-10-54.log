2025-08-12 18:10:54,252 - INFO - Logger initialized and ready.
2025-08-12 18:10:55,697 - INFO - 语言已设置为 zh_CN。
2025-08-12 18:10:55,697 - INFO - --- New Session Started ---
2025-08-12 18:11:28,174 - INFO - <scan_source_folder>
2025-08-12 18:11:28,175 - INFO - <select_mod_prompt>
2025-08-12 18:11:28,175 - INFO -   [1] Australia & New Zealand Flavor Pack
2025-08-12 18:11:28,175 - INFO -   [2] Forts & Engineers
2025-08-12 18:11:28,175 - INFO -   [3] GORA UI
2025-08-12 18:11:28,175 - INFO -   [4] Hydra's More Agenda's
2025-08-12 18:11:28,175 - INFO -   [5] MODJAM2025
2025-08-12 18:11:28,175 - INFO -   [6] MODJAM2025 - 副本
2025-08-12 18:11:28,175 - INFO -   [7] Offer Convoy Contribution
2025-08-12 18:11:28,176 - INFO -   [8] Project Utopia - Extended Timeline - Technical Preview
2025-08-12 18:11:28,176 - INFO -   [9] Tech & Res
2025-08-12 18:11:28,176 - INFO -   [10] TEST-ANZAC
2025-08-12 18:11:28,176 - INFO -   [11] UKrework
2025-08-12 18:11:32,188 - INFO - <you_selected>
2025-08-12 18:11:32,189 - INFO - 
--- 正在获取Mod主题上下文 ---
2025-08-12 18:11:32,189 - INFO - 已识别Mod名称为: 'Offer Convoy Contribution'
2025-08-12 18:11:33,362 - INFO - 最终使用的主题上下文为: 'Offer Convoy Contribution'
2025-08-12 18:11:33,362 - INFO - <cleanup_start>
2025-08-12 18:11:36,101 - INFO - <cleanup_deleting>
2025-08-12 18:11:36,101 - INFO - <cleanup_success>
2025-08-12 18:11:45,835 - INFO - 
--- 启动 '首次翻译' 工作流，目标: Offer Convoy Contribution ---
2025-08-12 18:11:46,004 - INFO - OpenAI client initialized successfully, using model: gpt-5
2025-08-12 18:11:46,004 - INFO - <processing_metadata>
2025-08-12 18:11:46,005 - INFO - <translating_mod_name>
2025-08-12 18:11:46,609 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-12 18:11:46,610 - ERROR - <api_call_error>
Traceback (most recent call last):
  File "J:\V3_Mod_Localization_Factory\scripts\core\openai_handler.py", line 64, in translate_single_text
    response = client.chat.completions.create(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Drlin\AppData\Roaming\Python\Python312\site-packages\openai\_utils\_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Drlin\AppData\Roaming\Python\Python312\site-packages\openai\resources\chat\completions\completions.py", line 925, in create
    return self._post(
           ^^^^^^^^^^^
  File "C:\Users\Drlin\AppData\Roaming\Python\Python312\site-packages\openai\_base_client.py", line 1249, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Drlin\AppData\Roaming\Python\Python312\site-packages\openai\_base_client.py", line 1037, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-08-12 18:11:46,612 - INFO - <metadata_success>
2025-08-12 18:11:46,612 - INFO - <processing_assets>
2025-08-12 18:11:46,613 - INFO - <asset_copied>
2025-08-12 18:11:46,614 - INFO - <parsing_file>
2025-08-12 18:11:46,614 - INFO - 
--- 正在翻译为: 简体中文 ---
2025-08-12 18:11:46,877 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-12 18:11:46,877 - ERROR - <api_call_error>
Traceback (most recent call last):
  File "J:\V3_Mod_Localization_Factory\scripts\core\openai_handler.py", line 115, in _translate_chunk
    response = client.chat.completions.create(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Drlin\AppData\Roaming\Python\Python312\site-packages\openai\_utils\_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Drlin\AppData\Roaming\Python\Python312\site-packages\openai\resources\chat\completions\completions.py", line 925, in create
    return self._post(
           ^^^^^^^^^^^
  File "C:\Users\Drlin\AppData\Roaming\Python\Python312\site-packages\openai\_base_client.py", line 1249, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Drlin\AppData\Roaming\Python\Python312\site-packages\openai\_base_client.py", line 1037, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-08-12 18:11:46,878 - WARNING - <retrying_batch>
2025-08-12 18:11:49,171 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-12 18:11:49,171 - ERROR - <api_call_error>
Traceback (most recent call last):
  File "J:\V3_Mod_Localization_Factory\scripts\core\openai_handler.py", line 115, in _translate_chunk
    response = client.chat.completions.create(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Drlin\AppData\Roaming\Python\Python312\site-packages\openai\_utils\_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Drlin\AppData\Roaming\Python\Python312\site-packages\openai\resources\chat\completions\completions.py", line 925, in create
    return self._post(
           ^^^^^^^^^^^
  File "C:\Users\Drlin\AppData\Roaming\Python\Python312\site-packages\openai\_base_client.py", line 1249, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Drlin\AppData\Roaming\Python\Python312\site-packages\openai\_base_client.py", line 1037, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-08-12 18:11:49,172 - WARNING - <retrying_batch>
2025-08-12 18:11:53,487 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-08-12 18:11:53,487 - INFO - Retrying request to /chat/completions in 20.000000 seconds
2025-08-12 18:12:13,924 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-12 18:12:13,924 - ERROR - <api_call_error>
Traceback (most recent call last):
  File "J:\V3_Mod_Localization_Factory\scripts\core\openai_handler.py", line 115, in _translate_chunk
    response = client.chat.completions.create(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Drlin\AppData\Roaming\Python\Python312\site-packages\openai\_utils\_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Drlin\AppData\Roaming\Python\Python312\site-packages\openai\resources\chat\completions\completions.py", line 925, in create
    return self._post(
           ^^^^^^^^^^^
  File "C:\Users\Drlin\AppData\Roaming\Python\Python312\site-packages\openai\_base_client.py", line 1249, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Drlin\AppData\Roaming\Python\Python312\site-packages\openai\_base_client.py", line 1037, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-08-12 18:12:13,925 - INFO - <creating_fallback_file>
2025-08-12 18:12:13,925 - INFO - <fallback_file_created>
2025-08-12 18:12:13,925 - INFO - <workflow_completed>
